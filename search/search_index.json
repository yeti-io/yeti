{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"ARCHITECTURE/","title":"ETL Platform Architecture","text":"<p>This document provides a comprehensive overview of the ETL Platform architecture, design decisions, and implementation details.</p>"},{"location":"ARCHITECTURE/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> </ul>"},{"location":"ARCHITECTURE/#overview","title":"Overview","text":""},{"location":"ARCHITECTURE/#core-components","title":"Core Components","text":"Component Purpose Technology Pipeline Manager Pipeline lifecycle management Go, gRPC Processing Engine ETL execution engine Go, custom DAG engine Config Service Configuration management Go, gRPC Monitoring Service Metrics and observability Go, gRPC, Prometheus Loading Engine Stream data loading Go, Kafka CLI Tool (yetictl) Command-line interface Go, Cobra Web UI Web interface React/TypeScript"},{"location":"ARCHITECTURE/#technology-stack","title":"Technology Stack","text":""},{"location":"ARCHITECTURE/#backend-services","title":"Backend Services","text":"<ul> <li>Language: Go</li> <li>API: gRPC with gRPC-Gateway (REST)</li> <li>Framework: Standard library + specialized libraries</li> <li>Database: PostgreSQL</li> <li>Cache: Redis</li> <li>Message Queue: Apache Kafka</li> <li>Storage: S3-compatible (MinIO/Ceph)</li> </ul>"},{"location":"ARCHITECTURE/#frontend","title":"Frontend","text":"<p>\u0425\u0417</p>"},{"location":"ARCHITECTURE/#infrastructure","title":"Infrastructure","text":"<ul> <li>Container Runtime: Containerd</li> <li>Orchestration: Kubernetes</li> <li>Monitoring: Prometheus + Grafana or Signoz</li> <li>Tracing: Jaeger or Signoz</li> <li>Secrets: Vault or Kubernetes Secrets</li> </ul> <p>Last Updated: 2025-09-30 Version: 0.0.1 Maintainers: Zherdev Egor</p>"},{"location":"architecture/data-flow/","title":"Data Flow Architecture","text":""},{"location":"architecture/data-flow/#overview","title":"Overview","text":"<pre><code>graph TB\n    subgraph \"Data Sources\"\n        DB1[(Source Database)]\n        FILES[File Systems]\n        STREAMS[Message Streams]\n        APIs[HTTP APIs]\n        CLOUD[Cloud Storage]\n    end\n\n    subgraph \"ETL Platform\"\n        INGESTION[Ingestion Layer]\n        PROCESSING[Processing Layer]\n        STORAGE[Storage Layer]\n        MONITORING[Monitoring Layer]\n    end\n\n    subgraph \"Data Destinations\"\n        DWH[(Data Warehouse)]\n        LAKE[Data Lake]\n        CACHE[(Cache)]\n        ALERTS[Alert Systems]\n        DASHBOARDS[Dashboards]\n    end\n\n    DB1 --&gt; INGESTION\n    FILES --&gt; INGESTION\n    STREAMS --&gt; INGESTION\n    APIs --&gt; INGESTION\n    CLOUD --&gt; INGESTION\n\n    INGESTION --&gt; PROCESSING\n    PROCESSING &lt;--&gt; STORAGE\n    PROCESSING --&gt; MONITORING\n\n    STORAGE --&gt; DWH\n    STORAGE --&gt; LAKE\n    STORAGE --&gt; CACHE\n    MONITORING --&gt; ALERTS\n    MONITORING --&gt; DASHBOARDS\n\n    style INGESTION fill:#e3f2fd\n    style PROCESSING fill:#e8f5e8\n    style STORAGE fill:#fff3e0\n    style MONITORING fill:#fce4ec\n</code></pre>"},{"location":"architecture/data-flow/#pipeline-data-flow","title":"Pipeline Data Flow","text":""},{"location":"architecture/data-flow/#end-to-end-pipeline-execution","title":"End-to-End Pipeline Execution","text":"<pre><code>flowchart LR\n    subgraph \"Source Systems\"\n        PG[(PostgreSQL)]\n        MONGO[(MongoDB)]\n        KAFKA[Kafka Stream]\n        S3_SRC[S3 Files]\n        API_SRC[REST APIs]\n    end\n\n    subgraph \"Ingestion Stage\"\n        EXTRACT[Extract Operator]\n        VALIDATE_SRC[Source Validation]\n        BUFFER[Data Buffer]\n    end\n\n    subgraph \"Processing Stage\"\n        FILTER[Filter Operator]\n        TRANSFORM[Transform Operator]\n        ENRICH[Enrichment Operator]\n        AGGREGATE[Aggregate Operator]\n        VALIDATE_PROC[Data Validation]\n    end\n\n    subgraph \"Output Stage\"\n        FORMAT[Format Operator]\n        PARTITION[Partition Operator]\n        SINK[Sink Operator]\n    end\n\n    subgraph \"Destination Systems\"\n        DWH[(Data Warehouse)]\n        S3_DEST[S3 Data Lake]\n        KAFKA_DEST[Kafka Topics]\n        ELASTIC[Elasticsearch]\n        REDIS_DEST[(Redis)]\n    end\n\n    PG --&gt; EXTRACT\n    MONGO --&gt; EXTRACT\n    KAFKA --&gt; EXTRACT\n    S3_SRC --&gt; EXTRACT\n    API_SRC --&gt; EXTRACT\n\n    EXTRACT --&gt; VALIDATE_SRC\n    VALIDATE_SRC --&gt; BUFFER\n\n    BUFFER --&gt; FILTER\n    FILTER --&gt; TRANSFORM\n    TRANSFORM --&gt; ENRICH\n    ENRICH --&gt; AGGREGATE\n    AGGREGATE --&gt; VALIDATE_PROC\n\n    VALIDATE_PROC --&gt; FORMAT\n    FORMAT --&gt; PARTITION\n    PARTITION --&gt; SINK\n\n    SINK --&gt; DWH\n    SINK --&gt; S3_DEST\n    SINK --&gt; KAFKA_DEST\n    SINK --&gt; ELASTIC\n    SINK --&gt; REDIS_DEST\n\n    style EXTRACT fill:#e3f2fd\n    style TRANSFORM fill:#e8f5e8\n    style VALIDATE_PROC fill:#fff3e0\n    style SINK fill:#fce4ec\n</code></pre>"},{"location":"architecture/data-flow/#data-processing-patterns","title":"Data Processing Patterns","text":""},{"location":"architecture/data-flow/#batch-processing-flow","title":"Batch Processing Flow","text":"<pre><code>sequenceDiagram\n    participant Source as Data Source\n    participant IE as Ingestion Engine\n    participant PE as Processing Engine\n    participant CS as Config Service\n    participant Storage as Data Storage\n\n    Note over Source,Storage: Batch Processing Flow\n\n    Source-&gt;&gt;IE: Read data in batches\n    IE-&gt;&gt;PE: Submit batch for processing\n    PE-&gt;&gt;CS: Get processing configuration\n    CS--&gt;&gt;PE: Return config and schema\n\n    loop For each batch\n        PE-&gt;&gt;PE: Apply transformations\n        PE-&gt;&gt;PE: Validate data quality\n        PE-&gt;&gt;PE: Apply business rules\n    end\n\n    PE-&gt;&gt;Storage: Write processed batch\n    PE-&gt;&gt;IE: Confirm batch completion\n    IE-&gt;&gt;Source: Move to next batch\n</code></pre>"},{"location":"architecture/data-flow/#stream-processing-flow","title":"Stream Processing Flow","text":"<pre><code>sequenceDiagram\n    participant Stream as Message Stream\n    participant IE as Ingestion Engine\n    participant PE as Processing Engine\n    participant Window as Window Manager\n    participant Sink as Output Sink\n\n    Note over Stream,Sink: Stream Processing Flow\n\n    Stream-&gt;&gt;IE: Continuous message flow\n    IE-&gt;&gt;PE: Forward messages\n\n    loop Continuous Processing\n        PE-&gt;&gt;Window: Add to processing window\n        Window-&gt;&gt;Window: Accumulate messages\n\n        alt Window Complete\n            Window-&gt;&gt;PE: Trigger window processing\n            PE-&gt;&gt;PE: Apply aggregations\n            PE-&gt;&gt;Sink: Output window results\n        end\n    end\n</code></pre>"},{"location":"architecture/data-flow/#control-flow","title":"Control Flow","text":""},{"location":"architecture/data-flow/#pipeline-lifecycle-management","title":"Pipeline Lifecycle Management","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Created\n    Created --&gt; Validating : Submit Pipeline\n    Validating --&gt; Validated : DSL Valid\n    Validating --&gt; Invalid : DSL Invalid\n    Invalid --&gt; [*]\n\n    Validated --&gt; Scheduled : Schedule Created\n    Scheduled --&gt; Triggered : Trigger Fired\n    Triggered --&gt; Running : Resources Allocated\n\n    state Running {\n        [*] --&gt; Initializing\n        Initializing --&gt; Executing\n        Executing --&gt; Completing\n        Completing --&gt; [*]\n\n        Executing --&gt; Failed : Stage Failure\n        Failed --&gt; Retrying : Auto Retry\n        Retrying --&gt; Executing : Retry Attempt\n        Retrying --&gt; Failed : Max Retries Exceeded\n    }\n\n    Running --&gt; Completed : Success\n    Running --&gt; Failed : Failure\n    Running --&gt; Cancelled : User Cancellation\n\n    Completed --&gt; [*]\n    Failed --&gt; [*]\n    Cancelled --&gt; [*]\n\n    Failed --&gt; Scheduled : Schedule Active\n    Completed --&gt; Scheduled : Schedule Active\n</code></pre>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>This document provides a high-level overview of the ETL Platform architecture, covering system design.</p>"},{"location":"architecture/overview/#system-architecture","title":"System Architecture","text":"<pre><code>graph TB\n    subgraph \"User Layer\"\n        USER[Data Engineers]\n        CLI[etlctl CLI]\n        WEB[Web UI]\n        API[External APIs]\n    end\n\n    subgraph \"API Gateway\"\n        GW[Load Balancer / API Gateway]\n        AUTH[Authentication]\n    end\n\n    subgraph \"Core Services\"\n        PM[Pipeline Manager]\n        PE[Processing Engine]\n        CS[Config Service]\n        MS[Monitoring Service]\n        IE[Ingestion Engine]\n        SH[Scheduler]\n    end\n\n    subgraph \"Data Infrastructure\"\n        DB[(PostgreSQL&lt;br/&gt;Metadata)]\n        CACHE[(Redis&lt;br/&gt;Cache)]\n        MQ[Kafka&lt;br/&gt;Events]\n        S3[Object Storage&lt;br/&gt;Data Lake]\n        VAULT[Secrets Manager]\n    end\n\n    USER --&gt; CLI\n    USER --&gt; WEB\n    CLI --&gt; GW\n    WEB --&gt; GW\n    API --&gt; GW\n\n    GW --&gt; AUTH\n    AUTH --&gt; PM\n    AUTH --&gt; PE\n    AUTH --&gt; CS\n    AUTH --&gt; MS\n    AUTH --&gt; IE\n\n    PM --&gt; SH\n    SH --&gt; PE\n    PM &lt;--&gt; CS\n    PM &lt;--&gt; MS\n    PE &lt;--&gt; CS\n    IE &lt;--&gt; MQ\n\n    PM --&gt; DB\n    CS --&gt; DB\n    MS --&gt; DB\n    PM --&gt; CACHE\n    CS --&gt; VAULT\n    PE --&gt; S3\n\n    style PM fill:#e1f5fe\n    style PE fill:#e8f5e8\n    style CS fill:#fff3e0\n    style MS fill:#fce4ec\n    style IE fill:#f3e5f5\n    style SH fill:#FEEBE7\n</code></pre>"}]}